{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "for dirnames, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirnames, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see percentage of missing data\n",
    "missing = (train_data.isnull().count() - train_data.count()) / train_data.isnull().count() * 100\n",
    "missing = pd.DataFrame(data = { 'original': train_data.isnull().count() , 'missing' : train_data.isnull().count() - train_data.count(), '%' : missing})\n",
    "missing.sort_values(by= '%', ascending = False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see Age distribution once\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "train_data['Age'].hist(bins= 40, figsize= (5, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to explore the data wrt to Age and Sex a little bit\n",
    "# but before that lets prepare some data for plotting\n",
    "\n",
    "# finding the Age distribution of passengers who survived in Male and Female\n",
    "women = train_data[train_data['Sex'] == 'female']\n",
    "men = train_data[train_data['Sex'] == 'male']\n",
    "\n",
    "Survived_women_Age_dist = women[women['Survived'] == 1].Age.dropna()\n",
    "Survived_men_Age_dist = men[men['Survived'] == 1].Age.dropna()\n",
    "\n",
    "# finding the Age distribution of passengers who died in Male and Female\n",
    "Died_women_Age_dist = women[women['Survived'] == 0].Age.dropna()\n",
    "Died_men_Age_dist = men[men['Survived'] == 0].Age.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(nrows=1, ncols= 2, figsize = (10, 4))\n",
    "\n",
    "ax = sns.distplot(Survived_men_Age_dist, bins = 40, label= 'Survived', ax = axes[0], kde= False)\n",
    "ax = sns.distplot(Died_men_Age_dist, bins = 40, label = 'Not Survived', ax= axes[0], kde = False)\n",
    "ax.legend()\n",
    "ax.set_title('Male')\n",
    "\n",
    "ax = sns.distplot(Survived_women_Age_dist, bins = 30, label= 'Survived', ax= axes[1], kde= False)\n",
    "ax = sns.distplot(Died_women_Age_dist, bins= 30, label = 'Not Survived', ax= axes[1], kde= False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferences:\n",
    "# 1. men tends to have higher survival rate when they are 20-35 years of age\n",
    "# 2. females have higher survival rate alltogether than men\n",
    "# 3. females among themselves have high survival rate 14-40(approx)\n",
    "# 4. infants in both cases have higher probability of survival\n",
    "# 5. men have low survival when they are above 45 or between 5-17(approx)\n",
    "# IT LOOKS A GOOD IDEA TO CREATE A SEPERATE AGE GROUP FEATURE THUS SCALE THE DATA EVENLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how having number of relatives affects the survival rates, by adding it as a feature in a photocopy dataset.\n",
    "train_copy = train_data.copy()\n",
    "train_copy['relatives'] = train_data['SibSp'] + train_data['Parch']\n",
    "axes = sns.factorplot('relatives','Survived', \n",
    "                      data=train_copy,kind= 'point' , aspect = 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see here passengers with 1-3 relatives had higher chances of surival but it droped down for people with more than 3(except 6)\n",
    "passengers with 0 relative had only 30% of them survived.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how many were people with 0 relatives and also check the same for 6 to see if they are significant in number and lucky as well.\n",
    "train_copy['relatives'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems only handfull of people were there with 6 relatives, but a lot of people were traveling alone( 0 relatives) and out of which we see only around 30% of them survived. May be we could create a seperate feature of people traveling alone which decently affected their chances of survival. Let's keep this thing in mind for later and look at the correlation of each feature with the Survival rate using .corr() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr =  train_copy.corr()\n",
    "corr['Survived'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = train_data['Fare'].hist(bins = 20, label= 'Fare')\n",
    "plot.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fare has a strong correlation with Survival rate, but it also has very uneven distribution\n",
    "I want to create Fare groups of decent size but that would just distribute most of the Fares into few of the groups and rest groups would get very small number of Fares. We would look at a way to create an even distribution(buckets/groups) using great sklearn qcut() function after a little while. \n",
    "Passenger ID doesn't seems to be have much affect on survival.\n",
    "Parch and Sibsp have both positive and negetive correlation respectively. Better did we combine them above already to get more closer picture.\n",
    "We have already handles Age\n",
    "Now let's look at Pclass which is also giving inverse correlation with Survival rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Pclass', 'Survived', data= train_data, kind=  'bar', aspect= 1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot('Embarked', 'Survived', data= train_data, kind=  'bar', aspect= 1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see class 1 has a higher chances of survival and a passenger with class 3 has fairly lower survival rate. That's an already clear distinction to be left the same way. So let's move forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING: Formation of custom transformers along with a data Pipeline that would perform our custom transformation along with some inbuilt preprocessing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing any preprocessing to the data let's just keep an original copy of it separate.\n",
    "train_original = train_data.copy()\n",
    "train_data = train_data.drop(['Survived'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets have a look at the features before proceeding.\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we want to get the exact changes in our test set as well, so we will get ourself a custom transformer of our \n",
    "# own that we can call on test set as well later on. For now let's work on Training data.\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, DoTransform= True):\n",
    "        self.DoTransform = DoTransform\n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    def transform(self, X, y= None):\n",
    "            if self.DoTransform == True:\n",
    "                X['AgeBucket'] = (X['Age'] // 15 * 15).astype('Int64')\n",
    "                X['relatives'] = X['SibSp'] + X['Parch']\n",
    "                X['Not_Alone'] = X['relatives'].where(X['relatives'] == 0, 1)\n",
    "                X.drop(['Age'], axis = 1, inplace= True)\n",
    "                X['Fare'] = pd.qcut(X['Fare'], 6, labels = False)\n",
    "                return X\n",
    "            else:\n",
    "                return X\n",
    "\n",
    "# CustTran = CustomTransformer(DoTransform = True) 'Name', 'Ticket', 'Cabin',\n",
    "# train_data_tr = CustTran.transform(train_data)\n",
    "# train_data_tr\n",
    "\n",
    "# Above Statements are just to check if our transformer is working fine , we will call its object in through our Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we tried to pack everything in our transformer that cannot be using inbuilt classes. Now lets perform some transformations that can be handled by inbuilt classes such as handling the missing data and converting the categorical ones into numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets seperately list out our numerical and categorical attributes first which we need and remove what we don't need.\n",
    "train_num = train_data.drop(['Sex', 'Embarked', 'Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "num_attribs = list(train_num)\n",
    "train_cat = train_data.drop(num_attribs, axis = 1)\n",
    "train_cat = train_cat.drop(['Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "cat_attribs = list(train_cat)\n",
    "print(cat_attribs)\n",
    "print(num_attribs)\n",
    "train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a pipeline that will run our transformers - both for numerical and categorical attributes\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('custom_tr', CustomTransformer(DoTransform = True)),\n",
    "    ('impute', SimpleImputer(strategy= 'median')),\n",
    "    ])\n",
    "\n",
    "train_num_tr = num_pipeline.fit_transform(train_num)\n",
    "train_num_tr[0:5, :]\n",
    "\n",
    "# Above 2 lines of code are just to run the current cell to see if it's working fine. We will collectively run the whole \n",
    "# pipeline later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('Embarked', SimpleImputer(strategy= 'most_frequent')),\n",
    "    ('cat_encode', OneHotEncoder(sparse = False)),\n",
    "])\n",
    "train_cat_tr =  cat_pipeline.fit_transform(train_cat)\n",
    "train_cat_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We wil finally run both pipelines here using ColumnTransformer by passing numerical and categorical part of the data\n",
    "# whereever it's required\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num_transform', num_pipeline, num_attribs),\n",
    "    ('cat_transform', cat_pipeline, cat_attribs),\n",
    "    \n",
    "])\n",
    "final_train_data = full_pipeline.fit_transform(train_data)\n",
    "final_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's delete passengerID from the numerical data refering to its index using numpy.delete(arr, index, axis)\n",
    "# We didn't delete it in our custom transformer because we will run the same for test data as well, and we need\n",
    "# passengerID in the test set for submission to kaggle\n",
    "\n",
    "X_train = np.delete(final_train_data, 0, 1) \n",
    "print(X_train.shape)\n",
    "X_train[:5, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for y_train\n",
    "y_train = train_original['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be fitting the data to various machine learning models to see which performs good.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, oob_score=True, random_state = 42)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state = 42)\n",
    "dtc.fit(X_train, y_train)\n",
    "dtc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "svc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression()\n",
    "logr.fit(X_train, y_train)\n",
    "logr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "scores = cvs(rfc, X_train, y_train, cv = 4, scoring = 'accuracy')\n",
    "print(\"Scores: \", scores)\n",
    "print('Mean Score: ', scores.mean()) \n",
    "print(\"Std Dev: \", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty decent if not best, we have our best performing model a score of around over 80% with a standard deviation of 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the feature importance now that how each feature affect our model\n",
    "featureImportance = rfc.feature_importances_\n",
    "featureImportance = pd.DataFrame({'Features' : ['Pclass', 'SibSp', 'Parch', 'Fare', 'AgeBucket', 'Relatives', \n",
    "                                               'Not_Alone', 'Female', 'Male', 'C', 'Q', 'S'],\n",
    "                                 'Importance' : featureImportance}).sort_values(by = 'Importance', ascending = False)\n",
    "featureImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that 'C' , 'Q' and 'S' does not show to have any importance to the classifier. Also seems that Not Alone feature\n",
    "# is quite useless as well. I may want to drop those features from my training set to see if predictions imporves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.delete(X_train, [6, 9 ,10, 11], axis = 1)\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing it on our classifier again\n",
    "scores = cvs(rfc, X_train, y_train, cv = 10, scoring = 'accuracy')\n",
    "print(\"Scores: \", scores)\n",
    "print('Mean Score: ', scores.mean()) \n",
    "print(\"Std Dev: \", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that looks like an improvement! Now our model has over 82% accuracy with standard deviation of 3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning: using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gs_clf = GridSearchCV(estimator=rfc, param_grid=param_grid, n_jobs=-1)\n",
    "gs_clf.fit(X_train, y_train)\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = gs_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(final_model, 'final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We will run our pipeline on the test data: Remember, apart from the transformations done inside the pipeline we also did some transformations outside of the pipeline like deleting few Non Important features(based on their indices).\n",
    "everything shall be repeated for the test data as well in order to successfully run it through our pipeline.\n",
    "The no. of columns in our test data before running it through our saved pipeline should be exactly same as no. of columns there were in training data while it passed through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = test_data.copy()\n",
    "Y_test = full_pipeline.transform(test_data)\n",
    "Y_test = np.delete(Y_test, [0, 6, 9 ,10, 11], axis = 1)\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission according to compititon rules should be in csv format with exactly two cols: PassengerID of test set and our corresponding predictions. So I have just concatenated the PassengerID to my predictions and exported it as a new csv file for final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = final_model.predict(Y_test)\n",
    "final_predictions = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': final_predictions})\n",
    "joblib.dump(final_predictions, 'final_predictions')\n",
    "final_predictions.to_csv(r'final_submission.csv', index = False)\n",
    "joblib.dump(final_submission, 'final_submission')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'final_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all Folks :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
